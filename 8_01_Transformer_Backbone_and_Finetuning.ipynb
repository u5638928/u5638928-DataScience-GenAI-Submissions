{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u5638928/u5638928-DataScience-GenAI-Submissions/blob/main/8_01_Transformer_Backbone_and_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "s_TET4SyE2wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.01 Transformers: Backbone and Fine-tuning\n",
        "\n",
        "## DO THIS FIRST!!\n",
        "You will need to use a Hugging Face token to make this work. Follow these steps:\n",
        "1. Got to https://huggingface.co/\n",
        "2. Click \"Sign Up\" in the top right corner.\n",
        "3. Do the usual account sign up steps.\n",
        "4. Make sure you go to your email and click on the link to confirm your account.\n",
        "5. Once logged-in, click on your icon in the top right corner and select \"Access tokens\" (right at the bottom of the menu).\n",
        "6. Click \"+ Create new token\".\n",
        "7. Give your token a name and then scroll to the bottom to click \"Create\". You can ignore all the other options.\n",
        "8. Copy your token secret (\"hf_...\") and save it somewhere on your machine (e.g. Word or Notepad).\n",
        "9. Back in Colab, click on the key icon on the left hand side.\n",
        "10. Click on \"+ Add new secret\".\n",
        "11. Give the new secret the Name HF_TOKEN (please copy exactly this name).\n",
        "12. Paste in your token secret from step 8 as the Value.\n",
        "13. Make sure Notebook access is slide to the right. If done it will go blue and show a tick.\n",
        "15. Read on!"
      ],
      "metadata": {
        "id": "6O4GkckQH8BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lecture we have seen the overall architecture and approach of transformers. We have also seen that in general transformers have allowed us to build very large-scale models of billions of paramaters (maybe even trillions).\n",
        "\n",
        "How should we interact with such models? It is obviously difficult for us to train our own transformer of this scale, and if we train a much smaller model, can it compete?\n",
        "\n",
        "Instead, we may want to build on top of a transfomer trained at web-scale. This Notebook will explore a couple of methods of doing this.\n",
        "\n",
        "First, let's install some packages and some data:"
      ],
      "metadata": {
        "id": "qYCPLrjSlzCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load Dataset and Tokeniser\n",
        "dataset = load_dataset(\"imdb\") # use the in-built IMDB review dataset"
      ],
      "metadata": {
        "id": "DkSL-vczc4eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used some standard libraries, most of which we have seen. We have also installed the IMDB dataset - a the name suggests a set of movie details and reviews.\n",
        "\n",
        "We will now start to use some transformer technology! We need to convert our data first of all to embeddings. We'll do so with the popular [BERT](https://arxiv.org/abs/1810.04805) model from Google:"
      ],
      "metadata": {
        "id": "INu0ErOsmuYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # we want the data to be the same size each time but some reviews are shorter\n",
        "    # if any are less than max_length, we will add zeros to the end of it until\n",
        "    # it is the same length as max_length (padding=\"max_length\")\n",
        "    # if any are longer than max_length (could happen in production), then\n",
        "    # we truncate it - i.e. delete any characters after max_length is reached.\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# tokenise the data (covert to embeddings)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "1ChUV7uNdAt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have tokenised our data using BERT ... i.e. converted the text documents to a vector format where each document is now a set of vectors that represents the semantic meaning of the text.\n",
        "\n",
        "We can use this feature extracted dataset in a variety of ways, two of which we will explore. The first is taking this as basically a feature engineered dataset, and training a simple prediction \"head\" on top of this. I.e. we will not attempt to train the transformer model at all - we will use it with the weights learned through the training regime Google used.\n",
        "\n",
        "Instead we will just use that as our backbone, with a simple, dense neural network on top that we will train. First, let's build that as a model:"
      ],
      "metadata": {
        "id": "SiQ1EBwtnYrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Embeddings-Only Model with Custom Head\n",
        "\n",
        "class CustomHeadModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels):\n",
        "        super(CustomHeadModel, self).__init__()\n",
        "        self.embedding_dim = embedding_dim # input\n",
        "        self.linear1 = nn.Linear(embedding_dim, 64) # linear layer\n",
        "        self.relu = nn.ReLU() # ReLU activation\n",
        "        self.linear2 = nn.Linear(64, num_labels) # linear layer - binary output\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        x = self.linear1(embeddings)\n",
        "        x = self.relu(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "# create a model from the pretrained embeddings\n",
        "model_embeddings = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# get the size of the embeddings (number of dimensions)\n",
        "embedding_dim = model_embeddings.config.hidden_size\n",
        "\n",
        "num_labels = 1 # Binary classification (positive: 0, negative: 1)\n",
        "\n",
        "# setup the custom head to take embeddings size as input and labels as output\n",
        "# note this time we are doing binary classification with two neurons to keep\n",
        "# num_labels as flexible and something we could change to a higher number of labels\n",
        "# this works the same way as output=1. Its slightly less efficient but fine\n",
        "custom_head = CustomHeadModel(embedding_dim, num_labels)"
      ],
      "metadata": {
        "id": "uGNE_vp-fakK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a similar codebase to the one we have seen before. The difference is essentialy at the input layer. Rather than feeding in raw data, we have fed in the embeddings from BERT.\n",
        "\n",
        "In our model the input layer is based on the shape of the embeddings we create with BERT and incorporate into our architecture via [AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html). We then add a two layer DNN with ReLU and 64 neurons in the first layer and then we build our output layer as one neuron."
      ],
      "metadata": {
        "id": "LliGIjE0ol6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings-only prediction (with training of the classification head)\n",
        "\n",
        "# Freeze the embeddings\n",
        "for param in model_embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_embeddings.to(device) # add embedding model to GPU\n",
        "custom_head.to(device) # add custom head to GPU\n",
        "\n",
        "optimizer = optim.AdamW(custom_head.parameters(), lr=5e-5, weight_decay=0.0005) # Only optimise the custom head\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# use a subset of 1,000 random records for demonstration\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "# Data is a list including:\n",
        "# 1. id of words\n",
        "# 2. attention mask\n",
        "# 3. label\n",
        "\n",
        "# Custom collate function to handle lists which DataLoader doesn't do automatically\n",
        "def custom_collate(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels}\n",
        "\n",
        "\n",
        "train_loader = DataLoader(small_train_dataset, shuffle=True, batch_size=16, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "dtMB0dO8dNd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the model we can set up for training. At the very start of the code, we have made the embeddings part of the model non-trainable (by setting these parameters as \"requires\\_grad = False\"). This means during training only the weights in the custom head will be updated in optimisation.\n",
        "\n",
        "We specify a loss function (BinaryCrossEntropy becuase we are using one neuron as output).\n",
        "\n",
        "We create a DataLoader as before. The only difference is we have a customer collate function because our embedding output will effectively be a list each time including id's, attention masks and labels. In the code above this, we load in a subset of the data (1,000 rows) to reduce training time."
      ],
      "metadata": {
        "id": "Z8_b6UfQqEQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "custom_head.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        epoch_loss = 0 # reset loss\n",
        "\n",
        "        # add data to GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # Labels need to be float and unsqueezed for BCEWithLogitsLoss\n",
        "        labels = batch['label'].to(device).float().unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad(): # Freeze the bert/embeddings model\n",
        "            embeddings = model_embeddings(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = custom_head(embeddings) # get logits for custom head\n",
        "        loss = criterion(logits, labels) # get loss for custom head\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {round(epoch_loss/len(train_loader), 4)}\")"
      ],
      "metadata": {
        "id": "1dD855X0dfgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is a before. We can see from the very epoch we can see a low level of loss, and it stays about the same through each of the epochs. What does this tell you?\n",
        "\n",
        "Now we can make predictions:"
      ],
      "metadata": {
        "id": "7YUTaKdfrbhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"This is a great movie!\"\n",
        "encoded_input = tokenizer(example_text, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    embeddings = model_embeddings(**encoded_input).pooler_output\n",
        "    logits = custom_head(embeddings)\n",
        "\n",
        "    # select the highest probability neuron as the prediction\n",
        "    predictions = torch.argmax(logits, dim=1) # dim 0 is neurons, dim 1 is probability of each\n",
        "\n",
        "    # print result\n",
        "    print_label = \"positive\" if predictions.item() == 0 else \"negative\"\n",
        "    print(f\"Prediction for '{example_text}': {print_label}\")"
      ],
      "metadata": {
        "id": "e6170GPMeuUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We predict a very basic example \"This is a great movie!\". Unsuprisingly, given the low level of loss, this is easy for our model and it predicts it correctly.\n",
        "\n",
        "Now let's try another approach - fine-tuning. In this we will train the whole model:"
      ],
      "metadata": {
        "id": "gV2_DyTIrs6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the Model with the Custom Head\n",
        "\n",
        "# Reset the custom head so it is untrained\n",
        "custom_head = CustomHeadModel(embedding_dim, num_labels)\n",
        "\n",
        "# combine the embeddings model and custom head into one model\n",
        "model_fine_tune = nn.Sequential(model_embeddings,custom_head)\n",
        "model_fine_tune.to(device) # add the combined model to GPU\n",
        "\n",
        "# this time optimise all parameters\n",
        "optimizer = optim.AdamW(model_fine_tune.parameters(), lr=5e-5, weight_decay=0.0005)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# use a subset of 1,000 records for demonstration\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "# Custom collate function to handle lists which DataLoader doesn't do automatically\n",
        "def custom_collate(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels}\n",
        "\n",
        "train_loader = DataLoader(small_train_dataset, shuffle=True, batch_size=16, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "wYXqpjvvfCxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have reset the model - reverted the weights of the custom head back to zero (actually random, but the same meaning).\n",
        "\n",
        "Now we build a new model, \"model\\_fine\\_tune\", with our embeddings and our reset, custom head. Note, we do not set any part of the model to \"frozen\" (i.e. \"requires\\_grad = False\") ... we will be training all of it.\n",
        "\n",
        "The last parts of the code just rebuild the DataLoader. If you are running the whole Notebook you don't actually need to do this, I've included it just in case you want to copy out for your own project.\n",
        "\n",
        "Now we can train as before:"
      ],
      "metadata": {
        "id": "M3J4V05lsB-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tune.train()\n",
        "for epoch in range(num_epochs): #train for 3 epochs\n",
        "  for batch in train_loader:\n",
        "    epoch_loss = 0\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device).float().unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Pass input_ids and attention_mask to the first module in the sequence (model_embeddings)\n",
        "    # and then pass its output to the next module (custom_head)\n",
        "    outputs = model_fine_tune[1](model_fine_tune[0](input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {round(epoch_loss/len(train_loader), 4)}\")"
      ],
      "metadata": {
        "id": "SU7JwktfiYzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything here is code as we've previously seen. We also see the training outcomes which ... are about the same as before (although with a much slower training process as we are also training the embedding model). We can also try a prediction:"
      ],
      "metadata": {
        "id": "EN7soBjTtC_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"This is a terrible movie!\"\n",
        "encoded_input = tokenizer(example_text, return_tensors=\"pt\").to(device) # covert example to tokens\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Pass input_ids and attention_mask to the first module in the sequence (model_embeddings)\n",
        "    # and then pass its output to the next module (custom_head)\n",
        "    # **encoded_input means unpack the list of items\n",
        "    outputs = model_fine_tune[1](model_fine_tune[0](**encoded_input).pooler_output) # Pass encoded_input as keyword arguments\n",
        "\n",
        "    # select the highest probability neuron as the prediction\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # print result\n",
        "    print_label = \"positive\" if predictions.item() == 0 else \"negative\"\n",
        "    print(f\"Prediction for '{example_text}': {print_label}\")"
      ],
      "metadata": {
        "id": "qcRH_XIfimgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a great prediction. How could we improve? What should we conclude about the two approaches?"
      ],
      "metadata": {
        "id": "GxM9D_YSuhrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze and Correct Prediction Logic: Explain that torch.argmax is not suitable for binary classification with a single output neuron when using BCEWithLogitsLoss. The correct way to get a binary prediction is to apply a sigmoid activation to the raw logits and then compare against a threshold (e.g., 0.5).\n",
        "Re-evaluate Embeddings-Only Prediction: Modify the prediction cell for the embeddings-only model (e6170GPMeuUT) to use torch.sigmoid(logits) > 0.5 for binary classification and observe the prediction for 'This is a great movie!'. This will ensure the prediction logic is correct for a single output neuron.\n",
        "Re-evaluate Fine-Tuning Prediction: Modify the prediction cell for the fine-tuning model (qcRH_XIfimgB) to use torch.sigmoid(outputs) > 0.5 for binary classification and observe the prediction for 'This is a terrible movie!'. This correction is crucial to properly evaluate the fine-tuned model's performance.\n",
        "Discuss Potential Improvements: Detail various strategies to improve the model's performance, including using more training data, increasing the number of training epochs, performing hyperparameter tuning (learning rate, batch size, weight decay), exploring more complex custom head architectures, and considering alternative pre-trained transformer models. Also, suggest improving the loss reporting during training to show an average epoch loss.\n",
        "Compare and Conclude on Approaches: Provide a comparison between the embeddings-only (feature extraction) and fine-tuning approaches. Discuss their respective advantages, disadvantages, and typical use cases, drawing conclusions based on the observed (and corrected) prediction results and general machine learning principles.\n",
        "Final Task: Summarize the key findings regarding the corrected prediction logic, the identified improvements, and the conclusions drawn from comparing the two transformer utilization approaches."
      ],
      "metadata": {
        "id": "IsYgVsyCg_hW"
      }
    }
  ]
}