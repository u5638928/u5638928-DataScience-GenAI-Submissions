{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u5638928/u5638928-DataScience-GenAI-Submissions/blob/main/6_01_logistic_regression_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.01 Logistic Regression in PyTorch\n",
        "This tutorial will first show you setting up a logistic regression, with a dataset we have used previously, in PyTorch. The aim is to give you a feel for how PyTorch works.\n",
        "\n",
        "We'll start with the stuff you've previously seen:"
      ],
      "metadata": {
        "id": "e7M4FXnHxhEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "gkHWk-4IxwYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code takes the sklearn breast_cancer dataset, splits into train and test and then finally scales the data. Now we just need to convert this data into PyTorch's required datatype _tensors_ (see 6\\_X if you want to understand more about tensors)."
      ],
      "metadata": {
        "id": "W7-BFu0bx1x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Print an example\n",
        "print(X_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DLh5YBYyKln",
        "outputId": "dd79aebf-890e-4cc5-ba6e-1ff319494f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0655, 0.2577, 0.0773, 0.0344, 0.4872, 0.4175, 0.7334, 0.2174, 0.5040,\n",
            "         0.6424, 0.0782, 0.1843, 0.0531, 0.0203, 0.2664, 0.6294, 0.7672, 0.6293,\n",
            "         0.6362, 0.2993, 0.0596, 0.2833, 0.0560, 0.0251, 0.5232, 0.4494, 1.0000,\n",
            "         0.6014, 0.5249, 0.5295],\n",
            "        [0.6562, 0.5702, 0.6742, 0.4894, 0.5549, 0.9034, 0.5827, 0.7435, 0.6556,\n",
            "         0.5059, 0.1877, 0.0890, 0.1723, 0.1394, 0.0863, 0.3405, 0.0978, 0.2968,\n",
            "         0.1885, 0.1520, 0.6579, 0.5720, 0.6203, 0.4628, 0.5293, 0.8029, 0.5415,\n",
            "         0.9976, 0.4993, 0.6219],\n",
            "        [0.0726, 0.1403, 0.0802, 0.0388, 0.2219, 0.2333, 0.1403, 0.1083, 0.6268,\n",
            "         0.4143, 0.1080, 0.4210, 0.0872, 0.0312, 0.2388, 0.2788, 0.1089, 0.2955,\n",
            "         0.6353, 0.1702, 0.0487, 0.1922, 0.0564, 0.0215, 0.1847, 0.1543, 0.1116,\n",
            "         0.1748, 0.3385, 0.2531],\n",
            "        [0.1449, 0.5245, 0.1429, 0.0758, 0.3967, 0.1814, 0.0557, 0.0803, 0.3897,\n",
            "         0.2807, 0.0500, 0.2504, 0.0349, 0.0184, 0.1862, 0.0608, 0.0273, 0.1183,\n",
            "         0.2557, 0.0596, 0.1305, 0.6175, 0.1194, 0.0576, 0.5347, 0.1236, 0.0899,\n",
            "         0.2109, 0.3635, 0.2241],\n",
            "        [0.1214, 0.1748, 0.1183, 0.0607, 0.5486, 0.2097, 0.0254, 0.0641, 0.8414,\n",
            "         0.4136, 0.1464, 0.2389, 0.1204, 0.0520, 0.1972, 0.0656, 0.0194, 0.1552,\n",
            "         0.6336, 0.1748, 0.0856, 0.1447, 0.0781, 0.0360, 0.3826, 0.0784, 0.0173,\n",
            "         0.0886, 0.3927, 0.2133]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have sorted out the data, we can specify our model. Note this is much more customisable in PyTorch, but that also means the code is more complicated:"
      ],
      "metadata": {
        "id": "tYPOH2T7ySli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))"
      ],
      "metadata": {
        "id": "SNRzxQRvyRuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a custom class we create called `LogisticRegression`, which inherits from PyTorch's `nn.Module` (i.e. it uses PyTorch's `nn.Module` as a template). This is the standard way to define models in PyTorch. The class encapsulates the structure and behavior of our logistic regression model.\n",
        "<br><br><br>\n",
        "__Step-by-step explanation:__\n",
        "\n",
        "1. `class LogisticRegression(nn.Module)`: This line defines the class and indicates that it's a subclass of `nn.Module`.\n",
        "2. `def __init__(self, input_dim)`: This is the constructor of the class (the thing that builds objects of this class). It takes the input_dim as an argument, which represents the number of features in the input data.\n",
        "3. `super(LogisticRegression, self).__init__()`: This line initialises the parent class (`nn.Module`), ensuring that all necessary setup is done. I.e. if we create an object of this class, then this step builds the actual object according to our template we are defining.\n",
        "4. `self.linear = nn.Linear(input_dim, 1)`: This line creates a linear layer (i.e. linear regression), which is the core component of our logistic regression model. It takes `input_dim` number of features as input and produces a single output (for binary classification).\n",
        "5. `def forward(self, x)`: This method defines the forward pass of the model, which is how input data is transformed into predictions.\n",
        "6. `return torch.sigmoid(self.linear(x))`: This line performs the following steps (steps 7 and 8):\n",
        "7. `self.linear(x)`: Applies the linear layer to the input x.\n",
        "8. `torch.sigmoid(...)`: Applies the sigmoid function to the output of the linear layer, producing a probability between 0 and 1. This probability represents the model's prediction for the given input.\n",
        "<br><br>\n",
        "Now we have defined the class (our model specification essentially), we can create an object of this class. As per the `def __init__(self, input_dim)` command above (step 2), when we build an object we need to specify the `input_dim` (number of features in the input)."
      ],
      "metadata": {
        "id": "z5HWdHFPyls9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "model = LogisticRegression(input_dim)"
      ],
      "metadata": {
        "id": "NxmLrfbF1d8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have built an object of the class template called _model_ much as we did with sklearn (the only difference is that sklearn had the class template already built for us so all we had to do was import it). We also need to pass a couple of hyperparameters - the loss function (BCE is short for [binary cross entropy](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)) and the optimiser we want to use (SGD is short for [stochastic gradient descent](https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html)). If interested in how these work in PyTorch (you don't have to be), follow the links for a technical explanation."
      ],
      "metadata": {
        "id": "SaQsYW-P1j7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "VrJ2Vczy1jNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With everything setup, we can begin training! Again this is a more manual process in PyTorch, but is still fundamentally the same:"
      ],
      "metadata": {
        "id": "clyJC6zz2W63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 1000 # specify 1000 epochs (full passes through the data)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # put the model object in train mode\n",
        "    optimizer.zero_grad() # reset the gradient (next week)\n",
        "    outputs = model(X_train) # pass the X_train data\n",
        "\n",
        "    # calculate loss as the comparison between predictions (y_hat) and\n",
        "    # real values (y) according to our loss function (criterion)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward() # backpropogate loss (next week)\n",
        "    optimizer.step() # update the parameters based on this round of training\n",
        "\n",
        "    # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjX1tYB72eRZ",
        "outputId": "274d5401-9e57-403e-bdca-97d5d62eb8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 0.6777\n",
            "Epoch [20/1000], Loss: 0.6747\n",
            "Epoch [30/1000], Loss: 0.6717\n",
            "Epoch [40/1000], Loss: 0.6688\n",
            "Epoch [50/1000], Loss: 0.666\n",
            "Epoch [60/1000], Loss: 0.6632\n",
            "Epoch [70/1000], Loss: 0.6605\n",
            "Epoch [80/1000], Loss: 0.6578\n",
            "Epoch [90/1000], Loss: 0.6551\n",
            "Epoch [100/1000], Loss: 0.6525\n",
            "Epoch [110/1000], Loss: 0.65\n",
            "Epoch [120/1000], Loss: 0.6474\n",
            "Epoch [130/1000], Loss: 0.6449\n",
            "Epoch [140/1000], Loss: 0.6424\n",
            "Epoch [150/1000], Loss: 0.64\n",
            "Epoch [160/1000], Loss: 0.6375\n",
            "Epoch [170/1000], Loss: 0.6351\n",
            "Epoch [180/1000], Loss: 0.6328\n",
            "Epoch [190/1000], Loss: 0.6304\n",
            "Epoch [200/1000], Loss: 0.6281\n",
            "Epoch [210/1000], Loss: 0.6258\n",
            "Epoch [220/1000], Loss: 0.6235\n",
            "Epoch [230/1000], Loss: 0.6212\n",
            "Epoch [240/1000], Loss: 0.619\n",
            "Epoch [250/1000], Loss: 0.6168\n",
            "Epoch [260/1000], Loss: 0.6146\n",
            "Epoch [270/1000], Loss: 0.6124\n",
            "Epoch [280/1000], Loss: 0.6102\n",
            "Epoch [290/1000], Loss: 0.6081\n",
            "Epoch [300/1000], Loss: 0.6059\n",
            "Epoch [310/1000], Loss: 0.6038\n",
            "Epoch [320/1000], Loss: 0.6017\n",
            "Epoch [330/1000], Loss: 0.5997\n",
            "Epoch [340/1000], Loss: 0.5976\n",
            "Epoch [350/1000], Loss: 0.5956\n",
            "Epoch [360/1000], Loss: 0.5936\n",
            "Epoch [370/1000], Loss: 0.5916\n",
            "Epoch [380/1000], Loss: 0.5896\n",
            "Epoch [390/1000], Loss: 0.5876\n",
            "Epoch [400/1000], Loss: 0.5857\n",
            "Epoch [410/1000], Loss: 0.5837\n",
            "Epoch [420/1000], Loss: 0.5818\n",
            "Epoch [430/1000], Loss: 0.5799\n",
            "Epoch [440/1000], Loss: 0.578\n",
            "Epoch [450/1000], Loss: 0.5761\n",
            "Epoch [460/1000], Loss: 0.5743\n",
            "Epoch [470/1000], Loss: 0.5724\n",
            "Epoch [480/1000], Loss: 0.5706\n",
            "Epoch [490/1000], Loss: 0.5688\n",
            "Epoch [500/1000], Loss: 0.567\n",
            "Epoch [510/1000], Loss: 0.5652\n",
            "Epoch [520/1000], Loss: 0.5634\n",
            "Epoch [530/1000], Loss: 0.5617\n",
            "Epoch [540/1000], Loss: 0.5599\n",
            "Epoch [550/1000], Loss: 0.5582\n",
            "Epoch [560/1000], Loss: 0.5565\n",
            "Epoch [570/1000], Loss: 0.5548\n",
            "Epoch [580/1000], Loss: 0.5531\n",
            "Epoch [590/1000], Loss: 0.5514\n",
            "Epoch [600/1000], Loss: 0.5498\n",
            "Epoch [610/1000], Loss: 0.5481\n",
            "Epoch [620/1000], Loss: 0.5465\n",
            "Epoch [630/1000], Loss: 0.5449\n",
            "Epoch [640/1000], Loss: 0.5433\n",
            "Epoch [650/1000], Loss: 0.5417\n",
            "Epoch [660/1000], Loss: 0.5401\n",
            "Epoch [670/1000], Loss: 0.5385\n",
            "Epoch [680/1000], Loss: 0.5369\n",
            "Epoch [690/1000], Loss: 0.5354\n",
            "Epoch [700/1000], Loss: 0.5339\n",
            "Epoch [710/1000], Loss: 0.5323\n",
            "Epoch [720/1000], Loss: 0.5308\n",
            "Epoch [730/1000], Loss: 0.5293\n",
            "Epoch [740/1000], Loss: 0.5278\n",
            "Epoch [750/1000], Loss: 0.5263\n",
            "Epoch [760/1000], Loss: 0.5249\n",
            "Epoch [770/1000], Loss: 0.5234\n",
            "Epoch [780/1000], Loss: 0.522\n",
            "Epoch [790/1000], Loss: 0.5205\n",
            "Epoch [800/1000], Loss: 0.5191\n",
            "Epoch [810/1000], Loss: 0.5177\n",
            "Epoch [820/1000], Loss: 0.5163\n",
            "Epoch [830/1000], Loss: 0.5149\n",
            "Epoch [840/1000], Loss: 0.5135\n",
            "Epoch [850/1000], Loss: 0.5121\n",
            "Epoch [860/1000], Loss: 0.5108\n",
            "Epoch [870/1000], Loss: 0.5094\n",
            "Epoch [880/1000], Loss: 0.5081\n",
            "Epoch [890/1000], Loss: 0.5067\n",
            "Epoch [900/1000], Loss: 0.5054\n",
            "Epoch [910/1000], Loss: 0.5041\n",
            "Epoch [920/1000], Loss: 0.5028\n",
            "Epoch [930/1000], Loss: 0.5015\n",
            "Epoch [940/1000], Loss: 0.5002\n",
            "Epoch [950/1000], Loss: 0.4989\n",
            "Epoch [960/1000], Loss: 0.4977\n",
            "Epoch [970/1000], Loss: 0.4964\n",
            "Epoch [980/1000], Loss: 0.4952\n",
            "Epoch [990/1000], Loss: 0.4939\n",
            "Epoch [1000/1000], Loss: 0.4927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that loss decreasing each time we print, which is what we would expect! But how does the module perform on test data?"
      ],
      "metadata": {
        "id": "j9LfcOsF4hku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd97eUwdxJw5",
        "outputId": "deae51f5-692f-441a-800f-777a86e23118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9123\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "model.eval() # switch to testing mode\n",
        "with torch.no_grad(): # turn off the gradient (stop training)\n",
        "    y_pred = model(X_test) # pass the X_test data\n",
        "\n",
        "    # if the output < 0.5 then class 0 and else class 1\n",
        "    y_pred = (y_pred >= 0.5).float()\n",
        "    accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
        "    print(f'Accuracy: {round(accuracy, 4)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy looks good! How about a confusion matrix?"
      ],
      "metadata": {
        "id": "kkAA3fUr5Xdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay as CM\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to NumPy arrays\n",
        "y_test_np = y_test.numpy()\n",
        "y_pred_np = y_pred.numpy()\n",
        "\n",
        "CM.from_predictions(y_test_np, y_pred_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "wV5nZppC5Wvv",
        "outputId": "c398a1fc-0db0-4253-a697-ead36263c3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x78b68f23e600>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same code as earlier in the module and we can see it gives us good results! (Although you should note results might vary different times you run it because of how we use random numbers in the process - don't be scared of randomness!!).\n",
        "\n",
        "Overall we should be able to see this is a very similar process. However, unlike in sklearn, this is a much more manual process in places. This means the code is a little more hard work, but gives us much more flexibility to define the model how we want. For logistic regression we are just following the standard template of the algorithm. However, as we move on into deep learning we will see this flexibility is basically the job ... the key hyperparameters are how we define these different layers and transformations."
      ],
      "metadata": {
        "id": "02t5nq306OyT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc55feb"
      },
      "source": [
        "This tutorial demonstrated how to implement logistic regression using PyTorch, contrasting it with previous scikit-learn examples. Key learnings include the process of data preparation for PyTorch, which involves converting data into torch.tensor format and unsqueeze ing target variables for correct dimensionality. You learned to define a custom model by inheriting from nn.Module, encapsulating the linear layer and sigmoid activation function. The process also covered the explicit definition of a loss function (nn.BCELoss) and an optimizer (optim.SGD). A significant takeaway is the manual control over the training loop, including forward pass, loss calculation, backpropagation with loss.backward(), and parameter updates with optimizer.step(), which offers greater flexibility compared to scikit-learn's more abstract approach. Finally, model evaluation involved switching to evaluation mode, making predictions without gradient calculation, and using metrics like accuracy and a confusion matrix to assess performance."
      ]
    }
  ]
}